{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081e93e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:33:52.439879Z",
     "start_time": "2022-12-08T15:33:52.430967Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import normalize\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import swifter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0663f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7cc83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:33:57.052289Z",
     "start_time": "2022-12-08T15:33:56.441331Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('../data/V_NOTE_MHAV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d795b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:33:59.952984Z",
     "start_time": "2022-12-08T15:33:59.944782Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_content(text):\n",
    "    pattern_date = r'\\s*\\w+\\s\\(\\s*[\\w]+[-]*\\w*[,]*\\s*[\\w.]*\\s*[\\w\\s]*\\w+[,]*\\s*\\w*[\\w.]*\\s*[-]*\\w*\\)\\s+\\d{4}/\\d{2}/\\d{2}\\s+\\d{2}:\\d{2}:\\s+'\n",
    "    pattern_patient = r'<toPt\\s*\\w*>'\n",
    "    \n",
    "    messages = re.split(pattern_date, text)\n",
    "    messages_final = []\n",
    "    messages_roles = []\n",
    "    \n",
    "    for message in messages:\n",
    "        if message.strip() == '':\n",
    "            continue\n",
    "        \n",
    "        role = 1 if '<mhav:' in message else 0\n",
    "        messages_roles.append(role)\n",
    "        \n",
    "        match = re.search(pattern_patient, message)\n",
    "        \n",
    "        message = message.strip() if match is None else message[match.end():].strip()\n",
    "        messages_final.append(message)\n",
    "    return messages_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0c971",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:43:06.889918Z",
     "start_time": "2022-12-08T15:34:00.556327Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data['EXTRACT_NOTE_TEXT'] = df_data.NOTE_TEXT.swifter.apply(lambda x: extract_content(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c830d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:43:07.121625Z",
     "start_time": "2022-12-08T15:43:06.965653Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_for_w2v = df_data[['PERSON_ID','NOTE_DATE','EXTRACT_NOTE_TEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa05508",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:43:07.221511Z",
     "start_time": "2022-12-08T15:43:07.124410Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_for_w2v = df_data_for_w2v.explode('EXTRACT_NOTE_TEXT').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99d66c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:43:07.230662Z",
     "start_time": "2022-12-08T15:43:07.225282Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df_data_for_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db93a1ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:43:07.237295Z",
     "start_time": "2022-12-08T15:43:07.233275Z"
    }
   },
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b48e77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:43:08.082569Z",
     "start_time": "2022-12-08T15:43:07.239596Z"
    }
   },
   "outputs": [],
   "source": [
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "#     annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "#         'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"english\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"english\", \n",
    "    \n",
    "    unpack_hashtags=False,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "#     dicts=[emoticons]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e716c5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:43:08.218773Z",
     "start_time": "2022-12-08T15:43:08.085140Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43718208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:43:08.227086Z",
     "start_time": "2022-12-08T15:43:08.221117Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace('\\n',' ').translate(str.maketrans(string.punctuation.replace('\\'',''), ' ' * (len(string.punctuation) - 1), string.digits))\n",
    "    text = text_processor.pre_process_doc(text)\n",
    "    # Remove stop words\n",
    "    filtered_text = [word for word in text if word not in stop_words and word != '\\'']\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97249e92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:37.352066Z",
     "start_time": "2022-12-08T15:43:08.229505Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_for_w2v['NOTE_WORD'] = df_data_for_w2v.EXTRACT_NOTE_TEXT.swifter.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47fd11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:37.481381Z",
     "start_time": "2022-12-08T15:44:37.358814Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_jw = pd.read_csv('./data/R37 vocabulary list with context sentence 2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da138b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:37.497738Z",
     "start_time": "2022-12-08T15:44:37.484371Z"
    }
   },
   "outputs": [],
   "source": [
    "first_name = set(vocab_jw[vocab_jw['First name'].isna() == False].WORD.to_list())\n",
    "last_name = set(vocab_jw[vocab_jw['Last name'].isna() == False].WORD.to_list())\n",
    "vunetid = set(vocab_jw[vocab_jw['VUNetID'].isna() == False].WORD.to_list())\n",
    "name = first_name.union(last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd935d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:37.507650Z",
     "start_time": "2022-12-08T15:44:37.500448Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_word = set(vocab_jw[vocab_jw['Keep'].isna() == False].WORD.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69667a28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:37.519320Z",
     "start_time": "2022-12-08T15:44:37.510200Z"
    }
   },
   "outputs": [],
   "source": [
    "word_map = dict(zip(vocab_jw[vocab_jw.mapping.isna() == False].WORD, vocab_jw[vocab_jw.mapping.isna() == False].mapping))\n",
    "mispell = set(word_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec7351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:37.527785Z",
     "start_time": "2022-12-08T15:44:37.521759Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_name(words):\n",
    "    words = ['<name>' if word in name else word for word in words]\n",
    "    words = ['<vunetid>' if word in name else word for word in words]\n",
    "    words = [word_map[word] if word in mispell else word for word in words]\n",
    "    words = [word for word in words if word not in drop_word]\n",
    "    return words               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e237f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:40.255206Z",
     "start_time": "2022-12-08T15:44:37.530701Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_for_w2v['NOTE_WORD'] = df_data_for_w2v.NOTE_WORD.swifter.apply(lambda x: process_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a1b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:40.434979Z",
     "start_time": "2022-12-08T15:44:40.258486Z"
    }
   },
   "outputs": [],
   "source": [
    "words = df_data_for_w2v.NOTE_WORD.to_list()\n",
    "words = [w for word in words for w in word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff9423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:40.441463Z",
     "start_time": "2022-12-08T15:44:40.437098Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: Two dictionaries, vocab_to_int, int_to_vocab\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    # sorting the words from most to least frequent in text occurrence\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # create int_to_vocab dictionaries\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f57455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:40.597994Z",
     "start_time": "2022-12-08T15:44:40.443394Z"
    }
   },
   "outputs": [],
   "source": [
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words)))) # `set` removes any duplicate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e95a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:41.345028Z",
     "start_time": "2022-12-08T15:44:40.600027Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e6092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:41.863179Z",
     "start_time": "2022-12-08T15:44:41.352006Z"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = Counter(int_words)\n",
    "#print(list(word_counts.items())[0])  # dictionary of int_words, how many times they appear\n",
    "\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "# p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "# discard some frequent words, according to the subsampling equation\n",
    "# create a new list of words for training\n",
    "train_words = [word for word in int_words if word_counts[word] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635c532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T05:18:56.702623Z",
     "start_time": "2022-12-08T05:18:56.668324Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_dist = pd.read_csv(\"freq_dist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464ed3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T03:18:43.558072Z",
     "start_time": "2022-12-09T03:18:43.507093Z"
    }
   },
   "outputs": [],
   "source": [
    "# print some stats about this word data\n",
    "print(\"Total words in text: {}\".format(len(train_words)))\n",
    "print(\"Unique words: {}\".format(len(set(train_words)))) # `set` removes any duplicate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb27d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:38:56.845897Z",
     "start_time": "2022-10-26T01:38:55.467029Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data_for_w2v['NOTE_WORD'] = df_data_for_w2v.NOTE_WORD.apply(lambda x : [vocab_to_int[i] for i in x if word_counts[vocab_to_int[i]] >= 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098965cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:38:56.900236Z",
     "start_time": "2022-10-26T01:38:56.848370Z"
    }
   },
   "outputs": [],
   "source": [
    "df_data_for_w2v['LEN'] = df_data_for_w2v.NOTE_WORD.apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f1b40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:38:56.928960Z",
     "start_time": "2022-10-26T01:38:56.903055Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_for_w2v = df_data_for_w2v[df_data_for_w2v['LEN'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59176a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:38:56.936450Z",
     "start_time": "2022-10-26T01:38:56.931405Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_context(words, idx, window_size=10):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size // 2 + 1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    context_words = words[start:idx] + words[idx+1:stop+1]\n",
    "    \n",
    "    return list(context_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac74351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:38:56.942792Z",
     "start_time": "2022-10-26T01:38:56.938538Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_pair(words):\n",
    "    pairs = []\n",
    "    for idx in range(len(words)):\n",
    "        context = get_context(words, idx)\n",
    "        pairs.extend(list(zip([words[idx]] * len(context),context)))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03a8e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:39:10.034915Z",
     "start_time": "2022-10-26T01:38:56.944866Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_data_for_w2v['WORD_PAIR'] = train_data_for_w2v.NOTE_WORD.swifter.apply(lambda x : word_pair(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce046b15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:39:10.052669Z",
     "start_time": "2022-10-26T01:39:10.036855Z"
    }
   },
   "outputs": [],
   "source": [
    "word_pairs = train_data_for_w2v.WORD_PAIR.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f86d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:39:10.469794Z",
     "start_time": "2022-10-26T01:39:10.065661Z"
    }
   },
   "outputs": [],
   "source": [
    "word_pairs = [wp for word_pair in word_pairs for wp in word_pair]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063370b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:39:10.477040Z",
     "start_time": "2022-10-26T01:39:10.471842Z"
    }
   },
   "outputs": [],
   "source": [
    "len(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647b3de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T05:18:56.753060Z",
     "start_time": "2022-12-08T05:18:56.742533Z"
    }
   },
   "outputs": [],
   "source": [
    "word_freqs = np.array(sorted([val for val in freqs.values() if val >= 5 / total_count ], reverse=True))\n",
    "unigram_dist = word_freqs/word_freqs.sum()\n",
    "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f064ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T15:44:41.919712Z",
     "start_time": "2022-12-08T15:44:41.901840Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_to_int = {word:idx for word, idx in vocab_to_int.items() if idx < 10683}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803fdc9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T03:20:10.711607Z",
     "start_time": "2022-12-09T03:19:07.321448Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "google = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddbbad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T03:20:10.768126Z",
     "start_time": "2022-12-09T03:20:10.715302Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame({'Word':[int_to_vocab[i] for i in list(set(train_words))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b8d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T03:20:10.774297Z",
     "start_time": "2022-12-09T03:20:10.770312Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_emb(x):\n",
    "    try:\n",
    "        return google[x]\n",
    "    except KeyError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c74e69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T03:20:10.877409Z",
     "start_time": "2022-12-09T03:20:10.777015Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df['embed'] = word_df.Word.swifter.apply(lambda x : get_emb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abad692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:40:01.616152Z",
     "start_time": "2022-10-26T01:40:01.608842Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df = word_df[word_df['embed'].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63530a3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T05:07:49.709649Z",
     "start_time": "2022-12-09T05:07:49.704467Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df['is_google'] = word_df.embed.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174e8e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T03:44:40.212982Z",
     "start_time": "2022-12-09T03:44:40.198572Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df['is_medi'] = word_df['Word'].isin(med_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180bb45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T05:07:47.361881Z",
     "start_time": "2022-12-09T05:07:47.357736Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_google(x):\n",
    "    if x['is_medi']:\n",
    "        return False\n",
    "    else:\n",
    "        return not x['is_google']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b9b6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T05:07:57.797902Z",
     "start_time": "2022-12-09T05:07:57.564147Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df['is_google'] = word_df.apply(lambda x: is_google(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9e516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T04:19:23.660417Z",
     "start_time": "2022-12-09T04:19:23.648369Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df['freq'] = [freqs[i] for i in range(10683)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370061b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T05:10:35.668653Z",
     "start_time": "2022-12-09T05:10:35.639541Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df['log_freq'] = word_df.freq.apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb4364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:40:01.644281Z",
     "start_time": "2022-10-26T01:40:01.618040Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = np.array(word_df.embed.to_list())\n",
    "nonmed_words = np.array(word_df.Word.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ef1c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:40:01.651399Z",
     "start_time": "2022-10-26T01:40:01.646289Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_similar(x):\n",
    "    word_vec = np.array(x)\n",
    "    normalize_word = word_vec / np.linalg.norm(word_vec)\n",
    "    normalize_vec = weights / np.linalg.norm(weights, axis=1, keepdims=True)\n",
    "    consine = np.dot(normalize_vec, np.expand_dims(normalize_word, axis=1))\n",
    "    values = np.sort(consine.squeeze())[::-1]\n",
    "    values = [i for i in values if i >= 0.5]\n",
    "    index = np.argsort(consine.squeeze())[::-1]\n",
    "    return nonmed_words[index][1:len(values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f913c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:40:01.658429Z",
     "start_time": "2022-10-26T01:40:01.653330Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_similar_cos(x):\n",
    "    word_vec = np.array(x)\n",
    "    normalize_word = word_vec / np.linalg.norm(word_vec)\n",
    "    normalize_vec = weights / np.linalg.norm(weights, axis=1, keepdims=True)\n",
    "    consine = np.dot(normalize_vec, np.expand_dims(normalize_word, axis=1))\n",
    "    values = np.sort(consine.squeeze())[::-1]\n",
    "    values = [i for i in values if i >= 0.5]\n",
    "    index = np.argsort(consine.squeeze())[::-1]\n",
    "    return values[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d211b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:44:35.785984Z",
     "start_time": "2022-10-26T01:40:01.660413Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df['similar'] = word_df.embed.swifter.apply(lambda x : most_similar(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8e5e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:02.323610Z",
     "start_time": "2022-10-26T01:44:35.788474Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df['cosine'] = word_df.embed.swifter.apply(lambda x : most_similar_cos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241637ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T03:28:43.387662Z",
     "start_time": "2022-12-09T03:28:43.285234Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "med_df = pd.read_csv('./data/medical_word_similar_06.csv',converters={'SIMILAR':ast.literal_eval, 'COS':ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fe99a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-09T03:44:09.035059Z",
     "start_time": "2022-12-09T03:44:09.030741Z"
    }
   },
   "outputs": [],
   "source": [
    "med_word = med_df.WORD.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f752b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:02.399365Z",
     "start_time": "2022-10-26T01:49:02.396351Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf115e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:02.624807Z",
     "start_time": "2022-10-26T01:49:02.401096Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df['cosine'] = word_df.cosine.swifter.apply(lambda x:  softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e48ceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:02.809523Z",
     "start_time": "2022-10-26T01:49:02.626548Z"
    }
   },
   "outputs": [],
   "source": [
    "word_df['similar'] = word_df.similar.swifter.apply(lambda x: [vocab_to_int[xx] for xx in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3723c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:02.841373Z",
     "start_time": "2022-10-26T01:49:02.811257Z"
    }
   },
   "outputs": [],
   "source": [
    "med_df['similar'] = med_df.SIMILAR.swifter.apply(lambda x: [vocab_to_int[xx] for xx in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137b267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:02.879364Z",
     "start_time": "2022-10-26T01:49:02.843179Z"
    }
   },
   "outputs": [],
   "source": [
    "med_df['cosine'] = med_df.COS.swifter.apply(lambda x:  softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dbe01c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:25.957735Z",
     "start_time": "2022-10-26T01:49:02.881181Z"
    }
   },
   "outputs": [],
   "source": [
    "similar_word = []\n",
    "cosine = []\n",
    "for i in range(10683):\n",
    "    if len(med_df[med_df['WORD'] == int_to_vocab[i]]) == 0:\n",
    "        try:\n",
    "            similar_word.append(word_df.loc[word_df['Word'] == int_to_vocab[i], 'similar'].values[0])\n",
    "            cosine.append(list(word_df.loc[word_df['Word'] == int_to_vocab[i], 'cosine'].values[0]))\n",
    "        except IndexError:\n",
    "            similar_word.append([])\n",
    "            cosine.append([])\n",
    "    else:\n",
    "        similar_word.append(med_df.loc[med_df['WORD'] == int_to_vocab[i], 'similar'].values[0])\n",
    "        cosine.append(list(med_df.loc[med_df['WORD'] == int_to_vocab[i], 'cosine'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c28349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:25.965293Z",
     "start_time": "2022-10-26T01:49:25.960795Z"
    }
   },
   "outputs": [],
   "source": [
    "similar_list = similar_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a14267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:25.975742Z",
     "start_time": "2022-10-26T01:49:25.967473Z"
    }
   },
   "outputs": [],
   "source": [
    "no_prior = [i for i, x in enumerate(similar_list) if len(x) == 0]\n",
    "prior = [i for i, x in enumerate(similar_list) if len(x) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94669a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:25.981850Z",
     "start_time": "2022-10-26T01:49:25.977988Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax_list = cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07b918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:26.007716Z",
     "start_time": "2022-10-26T01:49:25.983955Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([len(x)>0 for x in softmax_list]).to_csv('has_prior_knowledge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e70280",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:26.020594Z",
     "start_time": "2022-10-26T01:49:26.009750Z"
    }
   },
   "outputs": [],
   "source": [
    "gamma = {word: 1 / freqs[word] for word in word_counts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b86ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:26.029582Z",
     "start_time": "2022-10-26T01:49:26.022703Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, word_pairs, n_samples=5, n_similar=5):\n",
    "        self.word_pairs = word_pairs\n",
    "        self.n_samples = n_samples\n",
    "        self.n_similar = n_similar\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         t1 = time.time()\n",
    "        center, context = self.word_pairs[idx]\n",
    "        print\n",
    "        g = gamma[center]\n",
    "#         t2 = time.time()\n",
    "        noise = torch.zeros(self.n_samples, dtype=torch.int64)\n",
    "        # noise words\n",
    "        ni = 0\n",
    "        while ni < self.n_samples:\n",
    "            neg = torch.multinomial(noise_dist,1,replacement=True)\n",
    "            if neg == center or neg == context:\n",
    "                continue\n",
    "            noise[ni] = neg\n",
    "            ni += 1\n",
    "        # similar words\n",
    "#         t3 = time.time()\n",
    "        if len(similar_list[center]) > 0:\n",
    "            similar = similar_list[center][torch.multinomial(torch.Tensor(softmax_list[center]),1,replacement=True)]\n",
    "        else:\n",
    "            similar = center\n",
    "#         t4 = time.time()\n",
    "#         print('noise: ', t3 - t2)\n",
    "#         print('similar: ', t4 - t3)\n",
    "        return center, context, noise, similar, g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879f600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:26.036196Z",
     "start_time": "2022-10-26T01:49:26.031460Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(word_pairs)\n",
    "dataloader = DataLoader(dataset, batch_size=5, shuffle=True, num_workers=12, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263155c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:26.048881Z",
     "start_time": "2022-10-26T01:49:26.038552Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGramNeg(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        self.vocab_to_int = vocab_to_int\n",
    "        self.int_to_vocab = int_to_vocab\n",
    "        \n",
    "        # define embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab,n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab,n_embed)\n",
    "        \n",
    "        # Initialize both embedding tables with uniform distribution\n",
    "        self.in_embed.weight.data.uniform_(-1,1)\n",
    "        self.out_embed.weight.data.uniform_(-1,1)\n",
    "    def create_lookup_tables(words):\n",
    "        \n",
    "        word_counts = Counter(words)\n",
    "        # sorting the words from most to least frequent in text occurrence\n",
    "        sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        # create int_to_vocab dictionaries\n",
    "        int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "        vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "        return vocab_to_int, int_to_vocab\n",
    "    \n",
    "    def forward_input(self, input_words):\n",
    "        # return input vector embeddings\n",
    "        input_vector = self.in_embed(input_words)\n",
    "        return input_vector\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        # return output vector embeddings\n",
    "        output_vector = self.out_embed(output_words)\n",
    "        return output_vector\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples, noise_words):\n",
    "        noise_vector = self.out_embed(noise_words).view(batch_size,n_samples,self.n_embed)        \n",
    "        return noise_vector\n",
    "    \n",
    "    def most_similar(self, word, n_similarity):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model.to(device)\n",
    "        word_vec = model.in_embed(torch.tensor(self.vocab_to_int[word]))\n",
    "        normalize_word = word_vec / word_vec.norm()\n",
    "        weights = model.in_embed.weight.data\n",
    "        normalize_vec = weights / weights.norm(dim=1, keepdim=True)\n",
    "        cosine = normalize_vec.mm(normalize_word.unsqueeze(1))\n",
    "        values, indices = torch.sort(cosine.squeeze(),descending=True)\n",
    "        return dict(zip([self.int_to_vocab[x.item()] for x in indices[:n_similarity]], [y.item() for y in values[:n_similarity]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433cc77b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:26.058987Z",
     "start_time": "2022-10-26T01:49:26.051050Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExpNegLoss(nn.Module):\n",
    "'''PK-word2vec model Loss calcuation\n",
    "'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors, similar_input_vectors,similar_output_vectors, g):\n",
    "        \n",
    "#         start_nsl = time.time()\n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        similar_input_vectors = similar_input_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        similar_output_vectors = similar_output_vectors.view(batch_size, embed_size, 1)\n",
    "    \n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # similar input loss\n",
    "        similar_input_loss = torch.bmm(normalize(similar_input_vectors, dim=2), normalize(input_vectors,dim=1))\n",
    "        similar_input_loss =  similar_input_loss.squeeze()\n",
    "        \n",
    "        # similar output loss\n",
    "#         print(similar_output_vectors.size())\n",
    "#         print(output_vectors.size())\n",
    "#         print(similar_input_vectors.size())\n",
    "#         print(input_vectors.size())\n",
    "        \n",
    "        similar_output_loss = torch.bmm(normalize(output_vectors,dim=2), normalize(similar_output_vectors,dim=1))\n",
    "        similar_output_loss = similar_output_loss.squeeze()\n",
    "        \n",
    "        similar_loss = (1 - similar_input_loss) + (1 - similar_output_loss)\n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
    "        \n",
    "#         print(similar_loss.shape)\n",
    "#         print(noise)\n",
    "#         end_nsl = time.time()\n",
    "#         print('nsl:  ' + str(start_nsl-end_nsl))\n",
    "\n",
    "        # negate and sum correct and noisy log-sigmoid losses\n",
    "        # return average batch loss\n",
    "        return (-out_loss - noise_loss + alpha * g * similar_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce403576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:26.068060Z",
     "start_time": "2022-10-26T01:49:26.060757Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExpNegLossNC(nn.Module):\n",
    "'''Comparison Model without context vector regulation\n",
    "'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors, similar_input_vectors,similar_output_vectors, g):\n",
    "        \n",
    "#         start_nsl = time.time()\n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        similar_input_vectors = similar_input_vectors.view(batch_size, 1, embed_size)\n",
    "    \n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # similar input loss\n",
    "        similar_input_loss = torch.bmm(normalize(similar_input_vectors, dim=2), normalize(input_vectors,dim=1))\n",
    "        similar_input_loss =  similar_input_loss.squeeze()\n",
    "        \n",
    "        similar_loss = (1 - similar_input_loss)\n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
    "        \n",
    "#         print(similar_loss.shape)\n",
    "#         print(noise)\n",
    "#         end_nsl = time.time()\n",
    "#         print('nsl:  ' + str(start_nsl-end_nsl))     \n",
    "\n",
    "        # negate and sum correct and noisy log-sigmoid losses\n",
    "        # return average batch loss\n",
    "        return (-out_loss - noise_loss + alpha * g * similar_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190a56a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:26.078209Z",
     "start_time": "2022-10-26T01:49:26.070340Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExpNegLossND(nn.Module):\n",
    "'''Comparison Model without downsampling\n",
    "'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors, similar_input_vectors, similar_output_vectors, g):\n",
    "        \n",
    "#         start_nsl = time.time()\n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        similar_input_vectors = similar_input_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        similar_output_vectors = similar_output_vectors.view(batch_size, embed_size, 1)\n",
    "    \n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # similar input loss\n",
    "        similar_input_loss = torch.bmm(normalize(similar_input_vectors, dim=2), normalize(input_vectors,dim=1))\n",
    "        similar_input_loss =  similar_input_loss.squeeze()\n",
    "        \n",
    "        # similar output loss\n",
    "#         print(similar_output_vectors.size())\n",
    "#         print(output_vectors.size())\n",
    "#         print(similar_input_vectors.size())\n",
    "#         print(input_vectors.size())\n",
    "        \n",
    "        similar_output_loss = torch.bmm(normalize(output_vectors,dim=2), normalize(similar_output_vectors,dim=1))\n",
    "        similar_output_loss = similar_output_loss.squeeze()\n",
    "        \n",
    "        similar_loss = (1 - similar_input_loss) + (1 - similar_output_loss)\n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
    "        \n",
    "#         print(similar_loss.shape)\n",
    "#         print(noise)\n",
    "#         end_nsl = time.time()\n",
    "#         print('nsl:  ' + str(start_nsl-end_nsl))\n",
    "\n",
    "        # negate and sum correct and noisy log-sigmoid losses\n",
    "        # return average batch loss\n",
    "        return (-out_loss - noise_loss + 10000 * alpha * similar_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd7bb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:26.084518Z",
     "start_time": "2022-10-26T01:49:26.080223Z"
    }
   },
   "outputs": [],
   "source": [
    "a_list = [0.0001,0.00005,0.0005,0.001,0.1,0.00001,0.000001,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279a882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T01:49:26.099515Z",
     "start_time": "2022-10-26T01:49:26.091720Z"
    }
   },
   "outputs": [],
   "source": [
    "high_freq_word = list(range(1000))\n",
    "mid_freq_word = list(range(4842,5842))\n",
    "low_freq_word = list(range(9683,10683))\n",
    "prior_word = random.sample(prior, 2000)\n",
    "no_prior_word = random.sample(no_prior, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138465a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T02:38:11.341617Z",
     "start_time": "2022-10-26T02:38:11.332180Z"
    }
   },
   "outputs": [],
   "source": [
    "def udf_similar(model, word, n_similarity):\n",
    "    model = model.to('cpu')\n",
    "    word_vec = model.in_embed(torch.tensor(word))\n",
    "    normalize_word = word_vec / word_vec.norm()\n",
    "    weights = model.in_embed.weight.data\n",
    "    normalize_vec = weights / weights.norm(dim=1, keepdim=True)\n",
    "    cosine = normalize_vec.mm(normalize_word.unsqueeze(1))\n",
    "    values, indices = torch.sort(cosine.squeeze(),descending=True)\n",
    "#     return  list(zip([model.int_to_vocab[y.item()] for y in indices[1:n_similarity + 1]], [y.item() for y in values[1:n_similarity + 1]]))\n",
    "    return [y.item() for y in values[1:n_similarity + 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859f29b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T02:38:12.787830Z",
     "start_time": "2022-10-26T02:38:12.777210Z"
    }
   },
   "outputs": [],
   "source": [
    "cos_emb = pd.DataFrame(columns=['Frequency','Model','Mean Cosine Similarity','epoch'])\n",
    "cos_emb_pk = pd.DataFrame(columns=['Prior Knowledge','Model','Mean Cosine Similarity','epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54059fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T02:38:14.049313Z",
     "start_time": "2022-10-26T02:38:14.045031Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40757349",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-10-26T04:27:23.978Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for a in a_list:\n",
    "#     random.seed(17)\n",
    "    # instantiating the model\n",
    "    embedding_dim = 45\n",
    "    batch_size = 10000\n",
    "    alpha = a\n",
    "    n_samples = 5\n",
    "    model = SkipGramNeg(len(noise_dist), embedding_dim, noise_dist=noise_dist).to(device)\n",
    "    modelnc = SkipGramNeg(len(noise_dist), embedding_dim, noise_dist=noise_dist).to(device)\n",
    "    modelnd = SkipGramNeg(len(noise_dist), embedding_dim, noise_dist=noise_dist).to(device)\n",
    "    dataset = Dataset(word_pairs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=12, drop_last=True)\n",
    "\n",
    "    # using the loss that we defined\n",
    "    criterion1 = ExpNegLoss()\n",
    "    optimizer1 = optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion2 = ExpNegLossNC()\n",
    "    optimizer2 = optim.Adam(modelnc.parameters(), lr=0.005)\n",
    "    criterion3 = ExpNegLossND()\n",
    "    optimizer3 = optim.Adam(modelnd.parameters(), lr=0.005)\n",
    "    \n",
    "    epochs = 10\n",
    "\n",
    "    # train for some number of epochs\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # get our input, target batches\n",
    "        for input_words, target_words, noise_words, similar_words, g in dataloader:\n",
    "            \n",
    "            model = model.to(device)\n",
    "            modelnc = modelnc.to(device)\n",
    "            modelnd = modelnd.to(device)\n",
    "            \n",
    "            inputs, targets, noises, similars, g = torch.LongTensor(input_words), torch.LongTensor(target_words), torch.LongTensor(noise_words), torch.LongTensor(similar_words), torch.DoubleTensor(g)\n",
    "            inputs, targets, noises, similars, g = inputs.to(device), targets.to(device), noises.to(device), similars.to(device), g.to(device)\n",
    "\n",
    "            # final\n",
    "            input_vectors = model.forward_input(inputs)\n",
    "            output_vectors = model.forward_output(targets)\n",
    "            noise_vectors = model.forward_noise(batch_size, n_samples, noises)\n",
    "            similar_input_vectors = model.forward_input(similars)\n",
    "            similar_output_vectors = model.forward_output(similars)\n",
    "\n",
    "            # negative sampling loss\n",
    "            loss = criterion1(input_vectors, output_vectors, noise_vectors, similar_input_vectors, similar_output_vectors, g)\n",
    "\n",
    "            optimizer1.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer1.step()\n",
    "            \n",
    "            # NC\n",
    "            input_vectors = modelnc.forward_input(inputs)\n",
    "            output_vectors = modelnc.forward_output(targets)\n",
    "            noise_vectors = modelnc.forward_noise(batch_size, n_samples, noises)\n",
    "            similar_input_vectors = modelnc.forward_input(similars)\n",
    "            similar_output_vectors = modelnc.forward_output(similars)\n",
    "\n",
    "            # negative sampling loss\n",
    "            loss = criterion2(input_vectors, output_vectors, noise_vectors, similar_input_vectors, similar_output_vectors, g)\n",
    "\n",
    "            optimizer2.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            \n",
    "            # ND\n",
    "            input_vectors = modelnd.forward_input(inputs)\n",
    "            output_vectors = modelnd.forward_output(targets)\n",
    "            noise_vectors = modelnd.forward_noise(batch_size, n_samples, noises)\n",
    "            similar_input_vectors = modelnd.forward_input(similars)\n",
    "            similar_output_vectors = modelnd.forward_output(similars)\n",
    "\n",
    "            # negative sampling loss\n",
    "            loss = criterion3(input_vectors, output_vectors, noise_vectors, similar_input_vectors, similar_output_vectors, g)\n",
    "\n",
    "            optimizer3.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer3.step()\n",
    "        \n",
    "        m_hf = []\n",
    "        mnd_hf = []\n",
    "        \n",
    "        for w in high_freq_word:\n",
    "            m_hf.extend(udf_similar(model,w,10))\n",
    "            mnd_hf.extend(udf_similar(modelnd,w,10))\n",
    "            \n",
    "        cos_emb = cos_emb.append({'Frequency': 'High','Model': 'Proposed Model','Mean Cosine Similarity': mean(m_hf),'epoch':e},ignore_index=True)\n",
    "        cos_emb = cos_emb.append({'Frequency': 'High','Model': 'Without Downsampling','Mean Cosine Similarity': mean(mnd_hf),'epoch':e},ignore_index=True)\n",
    "        \n",
    "        m_mf = []\n",
    "        mnd_mf = []\n",
    "        \n",
    "        for w in mid_freq_word:\n",
    "            m_mf.extend(udf_similar(model,w,10))\n",
    "            mnd_mf.extend(udf_similar(modelnd,w,10))\n",
    "            \n",
    "        cos_emb = cos_emb.append({'Frequency': 'Mid','Model': 'Proposed Model','Mean Cosine Similarity': mean(m_mf),'epoch':e},ignore_index=True)\n",
    "        cos_emb = cos_emb.append({'Frequency': 'Mid','Model': 'Without Downsampling','Mean Cosine Similarity': mean(mnd_mf),'epoch':e},ignore_index=True)\n",
    "        \n",
    "        m_lf = []\n",
    "        mnd_lf = []\n",
    "        \n",
    "        for w in low_freq_word:\n",
    "            m_lf.extend(udf_similar(model,w,10))\n",
    "            mnd_lf.extend(udf_similar(modelnd,w,10))\n",
    "            \n",
    "        cos_emb = cos_emb.append({'Frequency': 'Low','Model': 'Proposed Model','Mean Cosine Similarity': mean(m_lf),'epoch':e},ignore_index=True)\n",
    "        cos_emb = cos_emb.append({'Frequency': 'Low','Model': 'Without Downsampling','Mean Cosine Similarity': mean(mnd_lf),'epoch':e},ignore_index=True)\n",
    "\n",
    "        m_pk = []\n",
    "        mnc_pk= []\n",
    "        \n",
    "        for w in prior_word:\n",
    "            m_pk.extend(udf_similar(model,w,10))\n",
    "            mnc_pk.extend(udf_similar(modelnc,w,10))\n",
    "            \n",
    "        cos_emb_pk = cos_emb_pk.append({'Prior Knowledge':'True','Model': 'Proposed Model','Mean Cosine Similarity': mean(m_pk),'epoch':e},ignore_index=True)\n",
    "        cos_emb_pk = cos_emb_pk.append({'Prior Knowledge':'True','Model': 'Without Context Vector','Mean Cosine Similarity': mean(mnc_pk),'epoch':e},ignore_index=True)\n",
    "        \n",
    "        m_npk = []\n",
    "        mnc_npk= []\n",
    "        \n",
    "        for w in no_prior_word:\n",
    "            m_npk.extend(udf_similar(model,w,10))\n",
    "            mnc_npk.extend(udf_similar(modelnc,w,10))\n",
    "            \n",
    "        cos_emb_pk = cos_emb_pk.append({'Prior Knowledge':'False','Model': 'Proposed Model','Mean Cosine Similarity': mean(m_npk),'epoch':e},ignore_index=True)\n",
    "        cos_emb_pk = cos_emb_pk.append({'Prior Knowledge':'False','Model': 'Without Context Vector','Mean Cosine Similarity': mean(mnc_npk),'epoch':e},ignore_index=True)\n",
    "        \n",
    "        print('epoch : ', e)\n",
    "#             # loss stats\n",
    "#             if steps % log_every == 0:\n",
    "#                 print(\"Epoch: {}/{}\".format(e+1, epochs))\n",
    "#                 print(\"Loss: \", loss.item()) # avg batch loss at this point in training\n",
    "        \n",
    "#     torch.save(model, './sample_1_similar/word2vec_cosine_a_{}_g_45.model'.format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c31a772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T03:53:14.208654Z",
     "start_time": "2022-10-26T03:53:14.203241Z"
    }
   },
   "outputs": [],
   "source": [
    "cos_emb.to_csv('no_downsampling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ce34e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T15:51:29.475791Z",
     "start_time": "2022-10-26T15:51:29.469590Z"
    }
   },
   "outputs": [],
   "source": [
    "def change_name(x):\n",
    "    if x == \"Proposed Model\":\n",
    "        return \"With down-weighting\"\n",
    "    else:\n",
    "        return \"Without down-weighting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582106ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T15:51:48.796084Z",
     "start_time": "2022-10-26T15:51:48.754644Z"
    }
   },
   "outputs": [],
   "source": [
    "cos_emb['Model'] = cos_emb.Model.swifter.apply(lambda x: change_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b34c4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-26T16:21:43.002453Z",
     "start_time": "2022-10-26T16:21:40.914782Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(10, 6.5))\n",
    "sns.set(rc={\"figure.dpi\":300, 'figure.figsize':(7,6)})\n",
    "sns.set_theme(style=\"ticks\", rc={'axes.edgecolor':'black'})\n",
    "ax = sns.lineplot(data=cos_emb, x=\"epoch\", y=\"Mean Cosine Similarity\", hue=\"Model\", style=\"Frequency\", ax=axs, palette=[\"#82b0d4\", \"#fc8072\"])\n",
    "axs.tick_params(axis='both', which='major', labelsize=18)\n",
    "axs.grid()\n",
    "axs.set_xlabel(\"Training epoch\",fontsize=18)\n",
    "axs.set_ylabel(\"Mean cosine similarity\", fontsize=18)\n",
    "plt.legend(fontsize=18, bbox_to_anchor=(0, 1.01, 1, 0.2), loc=\"lower left\", mode=\"expand\", ncol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d406cfa8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-23T06:42:27.854Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, './sample_1_similar/word2vec_cosine_a_01_ng.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d6cc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T05:09:11.270301Z",
     "start_time": "2021-09-16T05:09:11.260345Z"
    }
   },
   "outputs": [],
   "source": [
    "def udf_most_similar(model, word, n_similarity):\n",
    "    word_vec = model.in_embed(torch.tensor(model.vocab_to_int[word]))\n",
    "    normalize_word = word_vec / word_vec.norm()\n",
    "    weights = model.in_embed.weight.data\n",
    "    normalize_vec = weights / weights.norm(dim=1, keepdim=True)\n",
    "    cosine = normalize_vec.mm(normalize_word.unsqueeze(1))\n",
    "    values, indices = torch.sort(cosine.squeeze(),descending=True)\n",
    "    return dict(zip([model.int_to_vocab[x.item()] for x in indices[:n_similarity]], [y.item() for y in values[:n_similarity]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1407fb1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T19:09:39.069434Z",
     "start_time": "2021-09-16T19:09:39.055624Z"
    }
   },
   "outputs": [],
   "source": [
    "udf_most_similar(model.to('cpu'),'tamoxifen',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ca154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T02:06:04.931810Z",
     "start_time": "2021-09-02T02:05:24.892930Z"
    }
   },
   "outputs": [],
   "source": [
    "model.most_similar('tamoxifen',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a7825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
